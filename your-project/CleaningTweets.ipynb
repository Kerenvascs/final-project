{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\My Probook\n",
      "[nltk_data]     G2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast, json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# Import Punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>city</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York</td>\n",
       "      <td>@UberEats Promo Code never works for 50%. what...</td>\n",
       "      <td>2020-07-14 23:57:38+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/FashionsWeek/status/128318...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York</td>\n",
       "      <td>Yea they’re terrible smh. @UberEats @Uber_Support</td>\n",
       "      <td>2020-07-14 23:36:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/kensthetic_/status/1283183...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>New York</td>\n",
       "      <td>@UberEats when are you coming to upstate ny?</td>\n",
       "      <td>2020-07-14 23:33:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/Thebobover/status/12831828...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>New York</td>\n",
       "      <td>@diginn why does the dig inn app and @UberEats...</td>\n",
       "      <td>2020-07-14 23:30:41+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/Hot4TaterTots/status/12831...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>New York</td>\n",
       "      <td>For my brother to have his Uber job back! He h...</td>\n",
       "      <td>2020-07-14 23:04:47+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/Rayofshine69/status/128317...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_count      city                                               body  \\\n",
       "0            0  New York  @UberEats Promo Code never works for 50%. what...   \n",
       "1            1  New York  Yea they’re terrible smh. @UberEats @Uber_Support   \n",
       "2            2  New York       @UberEats when are you coming to upstate ny?   \n",
       "3            3  New York  @diginn why does the dig inn app and @UberEats...   \n",
       "4            4  New York  For my brother to have his Uber job back! He h...   \n",
       "\n",
       "                        date hashtags  \\\n",
       "0  2020-07-14 23:57:38+00:00      NaN   \n",
       "1  2020-07-14 23:36:33+00:00      NaN   \n",
       "2  2020-07-14 23:33:33+00:00      NaN   \n",
       "3  2020-07-14 23:30:41+00:00      NaN   \n",
       "4  2020-07-14 23:04:47+00:00      NaN   \n",
       "\n",
       "                                                link  geo  \n",
       "0  https://twitter.com/FashionsWeek/status/128318...  NaN  \n",
       "1  https://twitter.com/kensthetic_/status/1283183...  NaN  \n",
       "2  https://twitter.com/Thebobover/status/12831828...  NaN  \n",
       "3  https://twitter.com/Hot4TaterTots/status/12831...  NaN  \n",
       "4  https://twitter.com/Rayofshine69/status/128317...  NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df= pd.read_csv('old_tweets.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizer + Lematizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to tokenize the sentences\n",
    "def tokenize_cleaner(x):\n",
    "    x = str(x)\n",
    "    mylist =  word_tokenize(x)\n",
    "    return mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>city</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>link</th>\n",
       "      <th>geo</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York</td>\n",
       "      <td>@UberEats Promo Code never works for 50%. what...</td>\n",
       "      <td>2020-07-14 23:57:38+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/FashionsWeek/status/128318...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[@, UberEats, Promo, Code, never, works, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New York</td>\n",
       "      <td>Yea they’re terrible smh. @UberEats @Uber_Support</td>\n",
       "      <td>2020-07-14 23:36:33+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/kensthetic_/status/1283183...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Yea, they, ’, re, terrible, smh, ., @, UberEa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_count      city                                               body  \\\n",
       "0            0  New York  @UberEats Promo Code never works for 50%. what...   \n",
       "1            1  New York  Yea they’re terrible smh. @UberEats @Uber_Support   \n",
       "\n",
       "                        date hashtags  \\\n",
       "0  2020-07-14 23:57:38+00:00      NaN   \n",
       "1  2020-07-14 23:36:33+00:00      NaN   \n",
       "\n",
       "                                                link  geo  \\\n",
       "0  https://twitter.com/FashionsWeek/status/128318...  NaN   \n",
       "1  https://twitter.com/kensthetic_/status/1283183...  NaN   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [@, UberEats, Promo, Code, never, works, for, ...  \n",
       "1  [Yea, they, ’, re, terrible, smh, ., @, UberEa...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tokenized'] = tweets_df['body'].apply(tokenize_cleaner) \n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\My Probook\n",
      "[nltk_data]     G2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\My Probook G2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordnet is a lexical database for the English language that helps the script determine the base word. \n",
    "# You need the averaged_perceptron_tagger resource to determine the context of a word in a sentence\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Remove the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ubereats', 'promo', 'code', 'never', 'work', 'for', '50', 'what', 'do', 'i', 'do']\n"
     ]
    }
   ],
   "source": [
    "tweets_df['cleaned'] = tweets_df['tokenized'].apply(remove_noise)\n",
    "tweets_df.head(2)\n",
    "print(tweets_df['cleaned'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a lematize function to apply in the tokens\n",
    "\n",
    "# # Obs: Before running a lemmatizer, we need to determine the context for each word in the text. \n",
    "# # This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence.\n",
    "\n",
    "# def lemmatize_sentence(tokens):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_sentence = []\n",
    "#     for word, tag in pos_tag(tokens):\n",
    "#         if tag.startswith('NN'):\n",
    "#             pos = 'n'\n",
    "#         elif tag.startswith('VB'):\n",
    "#             pos = 'v'\n",
    "#         else:\n",
    "#             pos = 'a'\n",
    "#         lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "#     return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df['lematized'] = tweets_df['tokenized'].apply(lemmatize_sentence)\n",
    "# tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a colum only with strings to remove stop words\n",
    "# tweets_df['tweets_str'] = [','.join(map(str, l)) for l in tweets_df['lematized']]\n",
    "# # tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Applying Conditions and removing stopwords\n",
    "# stop = stopwords.words('english')\n",
    "# pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "\n",
    "# tweets_df['cleaned'] = tweets_df['tweets_str'].str.replace(pat, '')\n",
    "# tweets_df['cleaned'] = tweets_df['cleaned'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "# print(tweets_df['tweets_str'][0])\n",
    "# print(tweets_df['cleaned'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re, string\n",
    "# string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punct(text):\n",
    "#     text  = ''.join([char for char in text if char not in string.punctuation])\n",
    "#     text = re.sub('[0-9]+', '', text)\n",
    "#     return text\n",
    "\n",
    "# tweets_df['cleaned'] = tweets_df['cleaned'].apply(lambda x: remove_punct(x))\n",
    "# tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting into lits for modelling\n",
    "# tweets_df['tweets_cleaned'] = tweets_df['cleaned'].apply(lambda x: x.strip('()').split(','))\n",
    "# print(tweets_df['tweets_str'][0])\n",
    "# print(tweets_df['cleaned'][0])\n",
    "# print(tweets_df['tweets_cleaned'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.to_csv('UberEats.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
